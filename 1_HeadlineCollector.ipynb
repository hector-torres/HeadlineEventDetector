{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Headline Collector\n",
    "---\n",
    "\n",
    "## Intro\n",
    "This notebook contains the code for collecting headlines from existing news aggregators.\n",
    "The purpose is two-fold: to test the reliability of news aggregators in event detection,\n",
    "and to provide a corpus of headlines on which to perform feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "\n",
    "def headline_importer() -> dict:\n",
    "\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # retrieves top stories\n",
    "    top = gn.top_news()\n",
    "\n",
    "    return top"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Persistence\n",
    "\n",
    "This prepare to save the data to disk or to database, depending on the desired storage\n",
    "medium.\n",
    "\n",
    "### API to DataFrame\n",
    "\n",
    "This cell converts data from the API call into a pandas DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def headline_to_dataframe(headline_dict: dict):\n",
    "    \"\"\"\n",
    "    Converts data from the Google News API to a pandas DataFrame for analysis.\n",
    "\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "    # key/value pairs of column names from API to new data schema\n",
    "    column_names = {'published': 'headline_time_created', 'id': 'headline_id', 'title': 'headline_text',\n",
    "                 'title_detail': 'headline_language', 'source': 'headline_source', 'link': 'headline_url'}\n",
    "\n",
    "    # create new pandas DataFrame from API dictionary's 'entries' values\n",
    "    headline_df = pd.DataFrame.from_dict(headline_dict['entries'])\n",
    "\n",
    "    # keep only necessary columns\n",
    "    headline_df = headline_df[['published', 'id', 'title', 'title_detail', 'source', 'link']]\n",
    "\n",
    "    # overrides nested dictionaries in API values with Series object dervices from nest values\n",
    "    headline_df['title_detail'] = headline_df['title_detail'].apply(pd.Series)['language']\n",
    "    headline_df['source'] = headline_df['source'].apply(pd.Series)['title']\n",
    "\n",
    "    # renames columns to match new data schema\n",
    "    headline_df.rename(columns=column_names, inplace=True)\n",
    "    headline_df.set_index('headline_id', inplace=True)\n",
    "\n",
    "\n",
    "    return headline_df, column_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataFrame to File\n",
    "\n",
    "This cell checks to see if data in the DataFrame is already in our data file, and only saves data if it is\n",
    "not already present."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def dataframe_to_file(api_dataframe, column_names: dict) -> None:\n",
    "    \"\"\"\n",
    "    Checks if data in DataFrame is present in target file, and saves DataFrame data that is not already\n",
    "    present.\n",
    "    :param column_names:\n",
    "    :param api_dataframe: a DataFrame containing headline data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # imports existing data file, if it already exists: if it doesn't exist, skips to creating file\n",
    "    directory_path = os.path.join('data', 'output')\n",
    "    file_path = os.path.join(directory_path, 'headlines.csv')\n",
    "    if os.path.isfile(file_path):\n",
    "        # creates a DataFrame object out of the last 1000 rows in our data file\n",
    "        existing_dataframe = pd.read_csv(file_path, sep='\\t', index_col='headline_id')\n",
    "\n",
    "        # sort existing DataFrame by headline_time_created\n",
    "        existing_dataframe.sort_values(by='headline_time_created', axis=0, ascending=False,\n",
    "                                       inplace=True)\n",
    "        # limit to last 1000 rows\n",
    "        existing_dataframe = existing_dataframe.head(1000)\n",
    "        # compare the headline_id column in our data file DataFrame to the headline_id column in our\n",
    "        # incoming headline DataFrame and parse out only non-matched (new) rows\n",
    "        new_headlines_dataframe = api_dataframe.loc[api_dataframe.index.difference(existing_dataframe.index),]\n",
    "\n",
    "        # test print statements\n",
    "        print('number of headlines in CSV\\n' + str(existing_dataframe.shape[0]))        # currently 38 (0-37)\n",
    "        print('number of total headlines in incoming dataframe\\n' +\n",
    "              str(api_dataframe.shape[0]))\n",
    "        print('number of new headlines in incoming dataframe\\n' +\n",
    "              str(new_headlines_dataframe.shape[0]))\n",
    "        print('expected number of headlines in CSV after save\\n' +\n",
    "              str(existing_dataframe.shape[0] + new_headlines_dataframe.shape[0]))\n",
    "\n",
    "        # TODO: keeps adding header to CSV file\n",
    "        # appends the non-matched rows to our data file\n",
    "        new_headlines_dataframe.to_csv(file_path, sep='\\t', index_label='headline_id', index=True,\n",
    "                                       mode='a')\n",
    "\n",
    "    else:\n",
    "        api_dataframe.to_csv(file_path, sep='\\t',\n",
    "                             header=['headline_time_created', 'headline_text', 'headline_language',\n",
    "                                     'headline_source', 'headline_url'], index_label='headline_id',\n",
    "                             index=True, mode='a')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of headlines in CSV\n",
      "97\n",
      "number of total headlines in incoming dataframe\n",
      "38\n",
      "number of new headlines in incoming dataframe\n",
      "0\n",
      "expected number of headlines in CSV after save\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "# runner\n",
    "headlines = headline_importer()\n",
    "headlines_dataframe, columns = headline_to_dataframe(headlines)\n",
    "dataframe_to_file(headlines_dataframe, columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}